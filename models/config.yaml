# Configuration for open-source models used in GraphRAG system

# Embedding model (for code embeddings)
embedding:
  model_name: "microsoft/graphcodebert-base"
  # Alternative options:
  # - "microsoft/codebert-base"
  # - "sentence-transformers/all-MiniLM-L6-v2"
  # - "sentence-transformers/all-mpnet-base-v2"
  device: "cpu"  # or "cuda" if GPU available
  max_length: 512

# LLM for reasoning (open-source only)
llm:
  # Option 1: Use Hugging Face transformers (local inference)
  provider: "transformers"
  model_name: "microsoft/phi-2"  # Small, fast model
  # Alternative options:
  # - "mistralai/Mistral-7B-v0.1" (requires more RAM)
  # - "HuggingFaceH4/zephyr-7b-beta"
  # - "codellama/CodeLlama-7b-hf"
  device: "cpu"  # or "cuda" if GPU available
  max_length: 2048
  temperature: 0.7
  top_p: 0.9

  # Option 2: Use llama-cpp-python for GGUF models (uncomment if preferred)
  # provider: "llama-cpp"
  # model_path: "path/to/model.gguf"
  # n_ctx: 2048

# ChromaDB settings
chromadb:
  persist_directory: "./data/chromadb"
  collection_name_template: "methods_{project_name}"

# Joern settings
joern:
  cpg_dir: "./data/cpg"
  script_dir: "./joern_scripts"

